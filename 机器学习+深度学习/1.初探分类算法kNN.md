# Chap.2 初探 kNN 及机器学习流程

## 1. 算法概要

KNN算法又称K近邻算法，其核心思想就是**“排队”**。

给定训练数据集，对于待分类的样本，计算待预测样本和训练集中所有数据的距离， 将距离从小到大取前 K 个，则**哪个类别在前K个数据点中的数量最多，就认为待预测的样本属于该类别。**

![QQ截图20230106101419](C:\Users\14927\OneDrive - bupt.edu.cn\笔记\QQ截图20230106101419.png)

## 2. 代码（可见 `jupyter`)

```py
import numpy as np
import matplotlib.pyplot as plt

## 创建测试用例
raw_data_X = [[3.393533211, 2.331273381],
              [3.110073483, 1.781539638],
              [1.343808831, 3.368360954],
              [3.582294042, 4.679179110],
              [2.280362439, 2.866990263],
              [7.423436942, 4.696522875],
              [5.745051997, 3.533989803],
              [9.172168622, 2.511101045],
              [7.792783481, 3.424088941],
              [7.939820817, 0.791637231]
             ]
raw_data_y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]

# 引入预测对象
x = np.array([8.093607318, 3.365731514]) 

# 绘制散点图
plt.scatter(X_train[y_train==0,0], X_train[y_train==0,1], color='g')
plt.scatter(X_train[y_train==1,0], X_train[y_train==1,1], color='r')
plt.scatter(x[0], x[1], color='b'), plt.show()

## KNN 模拟实现过程

# 设定参数
K = 6

# 求出训练数据到该测试数据的欧氏距离
# 利用 np.sum 以及 py 语法糖，就很好求
distances = [sqrt(np.sum(x_train - x)**2) 
             for x_train in X_train]

# 搞出距离最近几个点的下标，确定前 k 个数据的label
nearest = np.argsort(distances) 
tp_k = [y_train[i] for i in nearest[:K]] 

# 丢进记数类，选出最多一项所对应的label
votes = Counter(topK_y)
return votes.most_common(1)[0][0]
```

## 3. 机器学习实操流程（见 KNN_self 和 KNN_in_sklearn）

1. 模型训练过程

![image-20230106122441865](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230106122441865.png)

对于KNN来说，没有传统意义上的模型训练过程。因此KNN的模型可以视作训练数据集。

我们可以直接调用 `sklearn` 库中的KNN模型，也可以在本地封装一个然后传入 `jupyter` 自己用.

### 2. 判断机器学习算法性能： `Train Test Split`

从训练数据集中取出一部分数据，作为测试性能的数据集。见 `ts-split_test.ipynb`

### 3.  超参数设置

1）超参数和模型参数

- 超参数：在算法运行前决定的参数（如 KNN 算法的 K）
- 模型参数：在算法过程中学习的参数（KNN 没有）

2）如何设置？ 运用经验法和领域知识法，或者利用网格搜索法

3）寻找KNN中的超参数：K

```py
best_score = 0.0
best_k = -1
for k in range(1, 11):
    knn_clf = KNeighborsClassifier(n_neighbors=k)
    knn_clf.fit(X_train, y_train)
    score = knn_clf.score(X_test, y_test)
    if score > best_score:
        best_k = k
        best_score = score
        
print("best_k =", best_k)
print("best_score =", best_score)
```

4）KNN中另一个隐藏的超参数：距离权重

<img src="C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230109002029788.png" alt="image-20230109002029788" style="zoom:80%;" />

如图，红色数据对绿色数据点的影响应该明显高于蓝色数据点。

可以通过**将距离的倒数设置为权重**，对其重新进行不同种类间的权重排序。可通过设置 `sklearn.neighbors.KNeighborsClf` 的 `weight = 'distance'` 参数。

*距离应该考虑广义的**闵式距离**，在 `sklearn` 的模型中对应着参数 `p=2`。

### 4. 数据归一化

1）如果不平衡数据之间的量纲（尺度），则两个数据点之间的距离则不好评判。

解决方案 1：通过**最值归一化**，将所有数据映射至 0~1 之间。最值归一化适用于拥有明显边界的数据集，而不适用于范围过于夸张的情形（如过大的收入差距）。

即对于原数据集中的$x$，可以如下映射至$x_{scale}$：
$$
\frac{x-x_{min}}{x_{max}-x_{min}}
$$
解决方案 2：**均值方差归一化**，把所有数据归一到均值为0、方差为1的分布中。适用于没有明显边界，存在极端数值的一般数据集，不会受极端数据的影响。

令 $S$ 为数据集的方差，$x_{mean}$ 为数据集的均值，则可如下映射至 $x_{scale}$：
$$
x_{scale} = \frac {x-x_{mean}}{S}
$$


2）测试数据集的归一化：**其归一化标准应参照训练数据集**，运用 $Mean_{train}$ 和 $S_{train}$ 对其进行归一化，应该数据归一化视作算法的一部分，并保存训练时样本的均值和方程。
$$
X_{scale} = \frac {(X_{test} - Mean_{~train})}{S_{~train}}
$$
利用 `Sklearn` 中的 `Scalar` 类进行归一化操作，可见 `Scalar_test.ipynb`.

### 5. KNN算法的缺点

- 效率低下，朴素算法时间复杂度 $O(m*n)$，也可以用数据结构优化；

- 高度数据相关，数据集万一有坏点就噶了；

- 预测结果不具有可解释性；

- **维数灾难：** 维数增加，看似相近的两点之间距离不断增大

  可以通过 PCA 降维的方式规避这种问题。
