## 梯度下降法

### 1. 概要

- 梯度下降法不是一种机器学习方法，而是一种基于搜索的最优化方法

- 它的作用是最小化一个损失函数。很多模型是没有数学解的，因此需要基于搜索的策略去寻找。

- 损失函数的梯度代表着其增大最快的方向，我们向反方向移动即可找到对应的最低点。



### 2. 算法介绍

对于损失函数$J$，定义学习率 $\eta$。在任意点，我们进行如下参数调整：
$$
\bigtriangleup \theta = -\eta \frac{\partial J}{\partial \theta}
$$
$\eta$ 是梯度下降法的一个超参数，它的取值将决定获取最优解的速度；如果取值不合适，甚至无法得到最优解。

搜索的过程中，可能找到的是局部最优解而非全局最优解。我们可以通过随机化初始点或者模拟退火的方式去避免落入局部最优解。

伪代码：

```py
eta = 0.1 # 设置学习率
eps = 1e-8 # 设置最小精度要求
theta = 0.0
while 1:
    grad = dJ(theta) # 求 theta 处的梯度
    last_theta = theta
    theta = theta - eta * grad
    if (abs(J(theta)- J(last_theta)) < eps) : break
    # 代入给定的损失函数 J 中判断梯度下降变化
    
print(theta, J(theta))
```



### 3. 多元线性回归

目标是使得 $J=\frac{1}{m}\sum_{i=1}^m(y^{(i)}-\hat{y}^{(i)})^2 = MSE(y, \hat{y})$ 尽可能小，由梯度下降法可知：
$$
\nabla J(\theta) 
= \begin{pmatrix}
\partial J/ \partial \theta_0 \\
\partial J/ \partial \theta_1  \\
...\\
\partial J/ \partial \theta_n  \\
\end{pmatrix} 
= \frac{2}{m} \begin{pmatrix}
\sum_{i=1}^m (X_b^{(i)} \cdot \theta - y^{(i)}) \cdot 1\\
\sum_{i=1}^m (X_b^{(i)} \cdot \theta - y^{(i)})\cdot X_1^{(i)} \\
...\\
\sum_{i=1}^m (X_b^{(i)} \cdot \theta - y^{(i)})\cdot X_n^{(i)}  \\
\end{pmatrix}
$$
运用向量化运算，我们可以将其视作：

$$
\begin{equation}
\begin{aligned}
\nabla J(\theta) 
&= \frac{2}{m} ((X_b\cdot \theta - y)^T \cdot X_b)^T \\
&=\frac{2}{m} X_b^T\cdot(X_b\cdot \theta - y)
\end{aligned}
\end{equation}
$$

则求梯度的函数可以直接写成：

```py
def dJ(theta, X_b, y):
    return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(y)
```



### 4. 数据归一化

用**向量化运算**的方式大幅降低了模型计算的时间，但是面对真实存在的数据时仍耗时耗力。其原因在于不同的特征之间量纲（或称作**衡量标注**）不同**，因此需要进行数据归一化处理。**

向量化和数据归一化的具体实现代码见 `Real-Gradient-Descent.ipynb` 。



### 5. 随机梯度下降法

我们之前的计算公式将每一个样本所造成的贡献都计算在内，被称作是**批量梯度下降法**。如果样本量非常大，则会非常耗时。

因此，我们随机取出一个样本 $X_b^{(i)}$，计算出其对应的方向。根据实验证明，即便有一定的精度误差，但是可以极大节省训练模型的时间。

为了控制随机梯度下降所导致的不收敛，我们设置**学习率随着学习次数的增加逐渐递减**，对每次梯度下降进行一定程度的控制。令$\eta = \frac{a}{i_{ter}+b}$，其中 `a,b` 均为随机梯度下降的超参数。

运用随机梯度下降法以及 `Sklearn` 自带的 `SGD` 类分析实际数据的实现代码见：`Stochastic-Gradient-Descent.ipynb` .

除此之外，还有一种结合了批量梯度下降以及随机梯度下降优点的**小批量梯度下降法**，即构造从样本中抽出 `k` 组的子矩阵，利用其进行分析。

