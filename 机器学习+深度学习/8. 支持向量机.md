## 支持向量机（Support Vector Machine）

### 1. 引入

对于一组二分类数据集，逻辑回归为其找到一组决策边界，将不同类别的样本分开。但将训练样本分开的决策边界不唯一，此时我们应该如何解决这种**不适定问题**呢。

![image-20230124215938786](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230124215938786.png)

直观上看，应该选定两类训练样本"正中间"作为决策边界，即上图加粗的线。因为该决策边界对训练样本**局部扰动的"容忍押性最好**（训练集的局限性或噪声的因素），即该决策边界所产生的分类结果是最**鲁棒的**，**对未知样本的泛化能力最强**。



### 2. 概述

从直观的角度看，该决策边界应该是离两类训练样本的距离都是最远的（该距离定义为**所有样本中最近那个点**的距离）。这些个最近的点也被称为样本集的**支持向量**，定义了样本集的边界。

从线代的角度看，我们如下描述决策边界的方程：
$$
\omega^T\cdot x+b = 0
$$
其中 $ w= (w_1;w_2;...;w_d)$ 为法向量，决定了超平面的方向；$b$ 为位移项，决定了超平面与原点之间的距离。

![image-20230124225839407](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230124225839407.png)

假设超平面 $(w,b)$ 可以将训练样本正确分类，等价于对 $(x_i,y_i)\in D$，满足：
$$
\left\{\begin{matrix}
\frac{\omega^T\cdot x_i + b}{||w||} \ge +d,~~y_i=+1 \\
\frac{\omega^T\cdot x_i + b}{||w||} \le -d,~~y_i=-1 
\end{matrix}\right.
$$
注意到我们令两种样本的标签为 $\pm1$，这便于后续公式的化简。这里简单做一个变换，即得：
$$
\left\{\begin{matrix}
\omega^T\cdot x_i + b \ge +1,~~y_i=+1 \\
\omega^T\cdot x_i + b \le -1,~~y_i=-1 
\end{matrix}\right.
$$
如上图所示，使得公式中等号成立的样本即为所谓的 **“支持向量”**。**两个异类支持向量**到超平面的距离之和，被称作两类训练样本的间隔 `margin`，公式即为：
$$
margin = \frac{2~|\omega^T\cdot x_i+b|}{||w||} =\frac{2}{||w||}
$$
欲找到具有**"最大间隔" $(maximum~margin)$ 的决策边界**，也就是要找到能满足上式中约束的参数$w$和$b$，使得 $margin$ 最大，由于 $||w||$ 在分母位置，等价于使得 $||w||^2$ 最小化。因此我们可重写为：
$$
Min\{~\frac12 ||w||^2~\} ~~s.t.~y_i(\omega^T\cdot x_i+b)\ge1
$$
对于有限定条件的最优化问题，我们通常使用拉格朗日乘子法求解。



### 2*. 数学推导

- 第一步，引入拉格朗日乘子 $\alpha_i\ge0$，得到对应的拉格朗日函数：

$$
L(\omega, b, \alpha) = \frac12||w||^2+\sum_{i=1}^m \alpha_i(~1-y_i(w^T\cdot x_i+b)~)
$$

- 第二步，令该函数关于 $\omega$ 和 $b$ 求偏导为零（取极小值），整理得：

$$
w = \sum_{i=1}^m \alpha_iy_ix_i, ~~ \sum_{i=1}^m \alpha_iy_i=0
$$

- 由**对偶问题的性质**，主问题等价于：

$$
\max_\alpha~\sum_{i=1}^m\alpha_i - \frac12\sum\sum \alpha_i\alpha_jy_iy_jx_i^Tx_j~
\\ s.t. \sum_{i=1}^m\alpha_iy_i =0, ~\alpha_i\ge0,~i=1,2,..,m
$$

解得 $\alpha_i$ 后代入，即可求出 `SVM` 模型：$f(x) = \sum\alpha_iy_ix_i^Tx+b$ .

而对于上述过程，也需要满足 $KKT$ 条件，即要求：

![image-20230125130437343](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230125130437343.png)

> 于是对任意训练样本 $(x_i, y_i)$ ，总有问 $\alpha_i = 0$ 或 $y_if(x_i) = 1$。若前者成立 ，则该样本将不会在求和过程中出现，也就不会对 $f(x)$ 有任何影响；若后者成立,  则所对应的样本点都将位于最大间隔边界上，是支持向量；
>
> 这表征了一个支持向量机的重要性质：训练完成后，大部分训练样本都不用保留，最终模型仅与支持向量有关。这体现了支持向量机 “解的稀疏性” 这一性质。




### 3. 软间隔与正则化

对于 `Hard Margin SVM` 模型，我们假定**训练样本在样本空间中是线性可分的**。即存在一个决策边界能将不同类的样本完全划分开 ，然而在现实任务往往很难这一点；退一步说，即使恰好找到了某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果**不是由于过拟合所造成的（即泛化能力较低）**。

缓解该问题的办法是**允许支持向量机在某些样本上出错**。为此引入 **"软间隔"** `(soft margin)` 的概念，将原先的约束条件修改为 $y_i(\omega^T\cdot x_i+b)\ge 1-\varepsilon_i$ ，其中 $\varepsilon_i\ge0$ 表示**样本 $i$ 的容错范围**。

![image-20230125001518339](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230125001518339.png)



为了表征 **容错范围不宜过大** 这一事件，我们将其放入目标函数中考量，即：
$$
\min_\alpha~\frac12 ||w||^2 +~C\sum_{i=1}^m \ell_{s}(\varepsilon_i) ~
$$
其中：

- $C>0$ 是一个常数，描述容错范围在算法所占的比重。当 $C$ 为无穷大时，等价于强迫所有样本满足`Hard Margin`；

- $l_s$ 是损失函数，通常取 `0/1` 损失函数或 `hinge` 损失函数，具体含义见下图：

![image-20230125003115551](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230125003115551.png)

我们还可以换成别的替代损失函数以得到其他学习模型，它们具有一个共性：优化目标函数中第一项用来描述划分决策边界的"间隔"大小，另一项 $\sum l_s(\varepsilon_i)$ 用来表述训练集上的误差，又称 **“结构风险” 与 “经验风险”**。

从经验风险最小化的角度来说，我们可以将前者视为正则化项，C 即为正则化常数。其中，$L_p$ 范数即为常用的正则化项，$L_2$ 范数倾向于 $\omega$ 取值均衡，非零个数稠密；而 $L_{0/1}$ 范数倾向于 $\omega$ 分量稀疏，非零个数少。

关于超平面的可视化演示以及 `Hard SVM` 与 `Soft SVM` 的相关内容，可见代码 `svm_test.ipynb`.



### 4. 多项式特征与核函数

借助 `sklearn` 中的 `Polynomial` 和 `Pipeline` 即可为 `LineSVM` 添加多项式特征；咱也可以借助多项式核函数的 SVM 添加多项式特征，这种方式不直接对样本数据进行多项式修饰，而是内置于**核函数**之中。

![image-20230125124205606](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230125124205606.png)

传统的多项式特征法的思想是将样本**从原始空间射到一个更高维的特征空间，使得样本在这个特征空间内线性可分**。 如图便是将二维空间向三维的特征空间映射的过程。，如果原始空间有限维，则一定存在高维特征空间使样本可分。

令 $\phi(x)$ 表示 $x$ 映射后的特征向量，则特征空间的超平面模型可表示为：
$$
f(x) = \omega^T\cdot \phi(x) + b
$$
在之前的数学推导中，我们将问题转换为其对偶问题：
$$
\max_\alpha~\sum_{i=1}^m\alpha_i - \frac12\sum\sum \alpha_i\alpha_jy_iy_j\phi(x_i)^T\phi(x_j)~
\\ s.t.~ \sum_{i=1}^m\alpha_iy_i =0, ~\alpha_i\ge0,~i=1,2,..,m
$$
然而计算 $\phi(x_i)^T\phi(x_j)$ 虽然对于一般的多项式特征还挺好算，但如果有无穷维那是算不出来的。为了避开这个障碍，可以设想如下函数：
$$
\kappa (x_i, ~x_j) = <\phi(x_i),~\phi(x_j)> =\phi(x_i)^T\phi(x_j)
$$
有了 $\kappa(·,·)$ 这样的 **核函数`(kernel funcion)`**，我们就不必计算高维甚至无穷维特征空间中的内积，上式可重写为：
$$
\max_\alpha~\sum_{i=1}^m\alpha_i - \frac12\sum\sum \alpha_i\alpha_jy_iy_j\kappa (x_i, ~x_j)~
$$
如何判定一个函数能不能作为核函数呢？我们规定，如果对于任意数据集 $D=\{x_1,x_2,..,x_m\}$，一个对称函数对应的核矩阵能描述其中任意两个样本间的距离（即矩阵是半正定的），则可以找到一个对应的映射 $\phi$，他就能作为核函数使用。

![image-20230125141613405](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230125141613405.png)

换言之，任何一个核函数都隐式地定义了一个称为**"再生核希尔伯特空间"** (`Reproducing Kernel Hilbert Space`，简称 `RKHS`）的特征空间，听上去很高大上但就是核矩阵所张成的那个特征空间。

然而，在特征映射的形式未知的时候，我们并不知道什么样的核函数是最合适的，而核函数也仅是隐式地定义了这个特征空间。于是，**"核函数选择"** 成为支持向量机的最大变数，若核函数选择不恰当，则意味着将样本映射到了一个不合适的特征空间，很可能导致性能不佳。

下表列出了几个常用的核函数：

![image-20230125141758757](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230125141758757.png)

> 此处多项式核函数还可写作： $K(x_i, ~x_j) = (x_i^Tx_j+b)^d$，其中 $b,d$ 为我们设置的参数。



### 4*. 高斯核函数（RBF 核）

参照高斯函数的形式，我们令高斯核函数为：
$$
g(x) = \frac1{\sigma\sqrt{2\pi}}\cdot &\exp{(-\frac{||x-\mu||^2}{2\sigma^2})}
\\ \Leftrightarrow \kappa(x_i, x_j) = &\exp{(-\frac{||x_i-x_j||^2}{2\sigma^2})}
$$
高斯核函数将每一个样本点映射到一个“无穷维”的特征空间，使得在该特征空间中线性可分。事实上对于一个 `m*n` 的样本集，高斯核函数可以将其张成一个 `m*m` 的特征空间。对于样本数量较少而特征阶数高的 NLP 领域，高斯核函数有着较为重要的作用。

高斯函数中的 $\sigma$ 表示着样本集的标准差，$\sigma$ 越小样本越集中；高斯核的 $\sigma$ 亦为如此。而 `sklearn` 中令 $\gamma =\frac1{2\sigma^2}$，则意味着 **$\gamma$ 越大，高斯分布越窄，模型过拟合；反之，则分布越宽，模型泛化能力越强。**

![image-20230125152907291](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230125152907291.png) ![image-20230125152913588](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230125152913588.png)

上图左侧是 `gamma=1` 训练出的模型所对应的超平面，右侧是 `gamma=100` 所得到的超平面。前者训练得具有一定的泛化性能，而后者明显出现了过拟合的现象。

有关多项式核函数以及高斯核函数的相关测试，见代码 `svm_test2.ipynb`。



### 5. 支持向量回归

回归问题的本质，是找一个超平面去拟合数据点。而 SVM 解决回归问题，拟合数据点的思路就是：寻找一个超平面，在其 $margin$ 区域内的点越多越好，同时引入超参数 $\epsilon$ 表征两个边界平面到至超平面的距离。

![image-20230125155602840](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230125155602840.png)

从形式化的语言分析，SVR 问题可表述为：
$$
\min_\alpha~\frac12 ||w||^2 +~C\sum_{i=1}^m \ell_{\epsilon}(f(x_i)-y_i) ~
$$
其中 C 为正则化常数， $l_\epsilon$ 代表一个不敏感损失函数（如下图）：
$$
l_\epsilon = \begin{cases}
0,~&if~|z|\le\epsilon; \\
|z|-\epsilon ~&otherwise.
\end{cases}
$$
随后引入松弛变量 $\varepsilon_i$ 与 $\hat{\varepsilon}_i$，表示对于该回归超平面上下所允许的最大不敏感距离。因此原式也可重写为：
$$
&\min_\alpha~\frac12 ||w||^2 +~C\sum_{i=1}^m \ell_{\epsilon}( \varepsilon_i + \hat{\varepsilon}_i) ~
\\
\\ s.t. &~f(x_i)-y_i\le \epsilon+\varepsilon_i,
\\ &~y_i-f(x_i)\le \epsilon+\hat{\varepsilon}_i,
\\ &\varepsilon_i\ge0,~\hat{\varepsilon}_i\ge0,~i=1..m
$$
推导过程同 $SVC$，用拉格朗日乘子法即可。代码实现上，同样可以用管道封装一个支持向量回归的函数：

```python3
def StandardLinearSVR(epsilon=0.1):
    return Pipeline([
        ('std_scale', StandardScaler()),
        # C, kernel, 等超参需要调节
        ('linear_svr', LinearSVR(epsilon=epsilon))
    ])

svr = StandardLinearSVR()
svr.fit(x_train, y_train)
```



### 6. 期盼：SVM 的正确使用方法

![image-20230125160807877](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230125160807877.png)
