## 线性回归算法

### 1. 优点

- 思想简单，实现容易
- 是许多非线性模型的基础
- 具有良好的可解释性

### 2. 概述

寻找标签和样本特征向量之间的线性关系，最大程度地进行拟合。

![image-20230110172004276](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230110172004276.png)

为了描述线性回归的拟合性，我们用 $$\sum_{i=1}^m (y^{(i)} -\hat{y}^{(i)})^2$$  **平方和的形式** 去描述两个数据点之间的差距。

代入 $\hat{y}^{(i)}=ax^{(i)}+b$ ，即找出一组$(a,b)$使得 $\sum(y^{(i)}-ax^{(i)}-b)^2$ 尽可能小。（可将其视作模型的**损失函数**）



### 2*. 机器学习的一般思路：

通过分析问题，确定问题的**损失函数或者效用函数**；通过**最优化**损失函数或效用函数，得到机器学习的模型。

如：线性回归，多项式回归，SVM，神经网络……



### 3. 最小二乘法

令 $J(a,b) = \sum(y^{(i)}-ax^{(i)}-b)^2$ ，则对 `a,b` 分别求导可得：
$$
\begin{cases}
b = \bar{y} - a\bar{x}\\
a = \frac{\sum_{i=1}^m ~(x^{(i)}y^{(i)}-x^{(i)}\bar{y})}{\sum_{i=1}^m(x^{(i)}x^{(i)}-\bar{x}x^{(i)})} 
\end{cases}
$$
其中，可以对 a 式整理，变换为：
$$
a = \frac{\sum_{i=1}^m ~(x^{(i)}-\bar{x})(y^{(i)}-\bar{y})}{\sum_{i=1}^m(x^{(i)}-\bar{x})^2}
$$
程序如下所示：

```py
import numpy as np
x = np.array([1,2,3,4,5])
y = np.array([1,3,2,4,5])

x_mean = np.mean(x), y = np.mean(y)
up = 0, dn = 0

for x_i, y_i in zip(x, y):
    up += (x_i - x_mean)*(y_i - y_mean)
    dn += (x_i - x_mean) ** 2
    
a = up/dn, b = y_mean - a*x_mean # 求出 a, b
```



### 4. 向量化运算

$\sum_{i=1}^m ~(x^{(i)}-\bar{x})(y^{(i)}-\bar{y})$ 该式可视为 $\sum_{i=1}^m w^{(i)}v^{(i)}$ 的形式，而这等价于 $w,v$ 两个向量作点乘。

因此可以利用 `np` 库里的向量化运算简化程序，还可以极大**提升性能**：

```py
x_mean = np.mean(x), y = np.mean(y)

# 直接上 np.dot
up = np.dot(x_train - x_mean, y_train - y_mean)
dn = np.dot(x.train - x.mean, x_train - x.mean)

a = up/dn, b = y_mean - a*x_mean # 求出 a, b
```

notebook 中封装了一个利用向量化运算的线性回归算法库。`LinearRegress_test.ipynb` 进行了简单的测试。



 ### 5. 衡量回归算法的性能

令测试数据集 $y_{test}$ 的大小为 $m$。

1）我们可以利用**均方误差（MSE）**衡量线性回归误差：$\frac{1}{m}\sum_{i=1}^m ~(y_{test}^{(i)}-\hat{y}_{test}^{(i)})^2$ 

2）也可以利用**平均绝对误差（MAE）**：$\frac{1}{m}\sum_{i=1}^m ~|y_{test}^{(i)}-\hat{y}_{test}^{(i)}|$ . 

3）但是对于不同数据集，数据所蕴含的意义不同，导致相互不具有可比性。引入 `R Squared` 指标：
$$
R^2 = 1-\frac{\sum_i(\hat{y}^{(i)}-y^{(i)})^2}{\sum_i (\bar{y}^{(i)}-y^{(i)})^2}=1-\frac{MSE(\hat{y},~y)}{Var(y)}
$$
分子表示我们模型预测的误差值，分母表示最坏情况（基准模型）下预测的误差值。

- $R^2$ 的范围在 0~1 之间，越大表示我们的模型预测的越好；
- 如果 $R^2<0$，则数据可能并不存在线性关系；

`preform_test.ipynb` 给出了一个对波士顿房产数据进行线性回归预测房价的程序，并对其运用不同指标进行评估。

 

### 6. 多元线性回归

对于有多个属性描述的数据集，我们也可以找到其对应的多元线性回归方程：
$$
\hat{y}^{(i)} = \theta_0 + \theta_1X_1^{(i)} + \theta_2X_2^{(i)}+...+ + \theta_nX_n^{(i)}
$$
在数据集 $X$ 的前面补充一维 $X_0$，使其恒等于 1，与 $\theta_0$ 相匹配。

我们令 $\hat{\theta} = ( \theta_0,\theta_1,....,\theta_n)$ ，$X_b = \begin{pmatrix}
 1 & X_1^{(1)} & ... & X_n^{(1)} \\
 1 & X_1^{(2)} & ... & X_n^{(2)} \\ ... & ...\\ 1 & X_1^{(m)} & ... & X_n^{(m)}
\end{pmatrix}$，则原式可表示为：$\hat{y} = X_b \cdot \theta$；



而要想训练一个多元线性回归模型，我们要求$\sum(y^{(i)}-\hat{y}^{(i)})^2$ 尽量小，即等价于使得 $(y-X_b\cdot\theta)^T(y-X_b\cdot\theta)$ 尽量小，由最小二乘法，我们推出 $\theta = (X_b^TX_b)^{-1}X_b^Ty$ . 其中 $\theta_0$ 作为截距不直接与属性联系，而每一个系数 $\theta_i$ 都与一个系数挂钩。

代码可参考 `playML/MulLinearRegression.py`，以及 `mulreg_test.ipynb`。

### 7. 优缺点评价

- 优点：具有可解释性，容易处理具有线性性的回归问题；
- 缺点：时间复杂度高 $O(n^3)$ 级别。
