## 分类算法评估

### 1. 引入

对于 `accuracy_score` 方法评估的模型，如果遇到数据极度偏斜（即某些标签的样本极其缺失）则不能客观评价算法的好坏。

首先针对二分类问题，首先进行**混淆矩阵**分析。我们在真实值已知的情况下，分析预测值的分布，即哪些数据结果是 positive，哪些结果是 negative。混淆矩阵具体可分为四格：

- **TN（True Negative）：**真实值Negative，预测Negative

- **FP（False Positive）：**真实值Negative，预测Positive

- **FN（False Negative）：**真实值Positive，预测Negative

- **TP（True Positive）：**真实值Positive，预测Positive

![image-20230124132622491](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230124132622491.png)

我们希望右对角线，即 `TN` 和 `TP` 的数量越多越好，



### 2. **精准率和召回率**

- 精准率 $precision=\frac{TP}{TP+FP}$：表示对于风险事件的预测，有多少次是准确的；

- 召回率 $recall=\frac{TP}{TP+FN}$ ：表示对于风险事件发生时，有多少是准确的；

用新冠做例子：精准率指在 100 次预测为阳性的患者中，实际为阳性的人数；而召回率指在 100 个阳性患者中，预测正确为阳性的人数。

可以直接调用 `sklearn` 中的混淆矩阵，精准率和召回率：

```py
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

confusion_matrix(y_test, log_reg_predict)
# array([[403,   2],
#        [  9,  36]])
precision_score(y_test, log_reg_predict) 
# 精准率：0.947
recall_score(y_test, log_reg_predict) 
# 召回率：0.8
```

有时候我们更注重精准率（如股票预测），有时候注重召回率（如病情预测）。但如果要同时兼顾精准率和召回率，可以引入 `F1 Score`：**即二者的调和平均值**（防止二者不平衡）：
$$
F1 = \frac{2\cdot precision\cdot recall}{precision+recall}
$$
同时我们也应该注意到，精准率和召回率本身是两个相互矛盾的指标：当**我调高逻辑回归的分类门槛时，精准率会不断提高，而召回率却有所下降；**反之同理。分类门槛：$\theta^T\cdot X_b = threshold$ .

在 `sklearn` 中，我们调用 `decision_function` 可以获取所有 `y_test` 对应的得分。通过调整分类门槛，我们可以平衡精准率和召回率：

```py
log_reg.decision_function(x_test)
decision_score =  log_reg.decision_function(x_test)

y_predict2 = np.array(decision_score >= 5, dtype='int')
# 原分类门槛为0，如此提高分类门槛
precision_score(y_test, y_predict2)
# 0.96
recall_score(y_test, y_predict2)
# 0.5333333333333333
```



甚至可以通过枚举 `thresholds` 的值，绘制**精准率与召回率曲线（P-R曲线）**。

![image-20230124155609580](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230124155609580.png)

![image-20230124155835283](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230124155835283.png)

**对于第二种 P-R 曲线，若一个学习器的 P-R 曲线被另一个学习器的曲线完全包住，则可断言后者的性能优于前者 。**这其实就是 P-R 曲线下的面积，也可以来衡量模型的优劣，但是一般情况下都会使用另外一种曲线下的面积，即 ROC 曲线。



### 3. ROC 曲线

ROC 曲线的纵轴是**"真正例率"** (True Positive Rate ，简称 TPR) ，横轴是**"假正例率"** (False Positive  Rate ，简称 FPR) ，两者分别定义为：
$$
TPR = \frac{TP}{TP+FN}, ~~FPR=\frac {FP}{TN+FP}
$$


为了提高 `TPR` 必须将分类门槛降低，相应的 `FPR` 错误也同时增高，因此 ROC 曲线呈现出向左上凸出的曲线形状。

与 P-R 图相似， 一个学习器的 ROC 曲线被另一个完全"包住"， 则可断言后者的性能优于前者；若两个学习 ROC 曲线发生交叉，则难以-般性地断言两者孰优孰劣。此时如果一定要比较，则较为合理的判据是**比较 ROC 线下的面积，即AUC (Area Under  ROC Curve).**

![image-20230124171717652](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230124171717652.png)



`sklearn` 库中也可以直接调用函数来计算一个训练器的 AUC 值。

```python3
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

fprs, tprs, thresholds = roc_curve(y_test, decision_scores)
plt.plot(fprs, tprs)
plt.show()
# 绘制 ROC 曲线

roc_auc_score(y_test, decision_scores)
# 0.9830452674897119
```



### 4. 多分类器的混淆矩阵

额，看网课睡过去了。