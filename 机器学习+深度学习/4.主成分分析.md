## 主成分分析（PCA）

### 1. 引入

- PCA 是一个非监督的机器学习算法；
- 用于数据的降维处理，从而利于可视化与去噪；

### 2. 如何姜维？

目的是姜维的同时**保持一定的区分度**。那么直接扔掉某些特征是否是最优的方案呢？

感觉不如找一个斜着的轴，使得样本间距最大（即数据的**方差**最大）：

![image-20230114220014692](C:\Users\14927\AppData\Roaming\Typora\typora-user-images\image-20230114220014692.png)



那对于一个数据集 $x_i$，我们将其映射至 $X_i$ 等价于使得$ Var(X) = \frac{1}{m}\sum (X_i-\bar{X})^2$ 最大。

我们运用 PCA 算法去优化寻找的过程。



### 3. 算法概述

- Step1：将样本的均值归零（`denmean`），此时 $Var(X) = \frac{1}{m}\sum ||X_i||^2$ .

- Step2：寻找一个单位向量 $\omega = (\omega_1,\omega_2,...,\omega_n)$，将样本映射至 $\omega$，得到对应的 $X_p$；

- 由线代知识可知，$||X_p|| = ||X||\cdot ||w|| \cdot cos\theta = X \cdot \omega$ ，即二者的内积；
- Step3：求 $\omega$， 使得函数 $Var(X_p) =\frac{1}{m}\sum (X_i\cdot\omega)^2$ 最大，运用**梯度上升法**即可；
- Step4：将数据**降维**，即高维数据向低维数据映射；



### 4. 梯度上升法

我们可以描述出 $f(X)=\frac{1}{m}\sum(X^{(i)}\cdot\omega)^2=\frac{1}{m}\sum_{i=1}^m(\sum_k X^{(i)}_k\cdot\omega_k)^2$ 的梯度向量：
$$
\nabla J(\theta) 
= \begin{pmatrix}
\partial J/ \partial \omega_0 \\
\partial J/ \partial \omega_1  \\
...\\
\partial J/ \partial \omega_n  \\
\end{pmatrix} 
= \frac{2}{m} \begin{pmatrix}
\sum_{i=1}^m (X^{(i)} \cdot \omega) \cdot X_1^{(i)}\\
\sum_{i=1}^m (X^{(i)} \cdot \omega) \cdot X_2^{(i)} \\
...\\
\sum_{i=1}^m (X^{(i)} \cdot \omega) \cdot X_n^{(i)} \\
\end{pmatrix}
$$
当然可以进一步进行向量化操作，简单想想就能推出来了（是一个 `1*m` 的矩阵乘上 `m*n` 的矩阵，再接一个转置啥的）。

最后可以得到：$\nabla J(X) = \frac{2}{m} \cdot X^T(X\omega)$ ；

具体代码实现可见  `PCA_test.ipynb` 和 `sklean_PCA_test.ipynb` 。



### 5. 数据降维 / 降噪处理

利用梯度上升法，当我们求出第一主成分之后，可以从所有样本中**减去第一主成分方向对应的分量**，再对于得到的 `X_pca` 运用新一轮的梯度上升，从而求出第二主成分。以此类推。

利用这种方法，我们可以求出前 k 个主成分，而每个主成分都是一个 n 维的方向向量。我们便可以将其写作矩阵的形式 $W_k$ . 此时，若想将原来 `m*n` 的 n 维数据集降至 k 维（k<n），则只需要令 $X_k = X \cdot (W_k)^T$ 即可。

代码可参考利用 PCA 为手写数字集 `MNIST` 进行降维处理，并采用 `KNN` 分类识别的例子。在实验的最后，我们发现利用PCA降维后的数据训练模型，其识别准确率不降反升。我们意识到 PCA 还可以对数据进行**降噪处理**。（可参考代码 `MNIST_test.ipynb`）

由公式可知，若想将 $X_k$ 恢复为 $X$，只需乘上一个 $(W_k)^T$ 的逆即可。虽然不可避免地会有数据丢失，但是只要丢失的数据大部分是**噪音**（即数据偏差），那就还是赚的。

在 `sklearn` 库里，我们只需调用如下代码，即可进行降噪操作。

```py
from sklearn.decomposition import PCA

... // 数据预处理

pca = PCA(0.8) // 假定可能有 20% 的噪音
pca.fit(X_train) 

X_train_reduc  = pca.transform(X_train)
X_train_filter = pca.inverse_transform(X_train_reduc) 
// 就先PCA一波，再套一个inv-PCA方法。
```



### 6. 特征脸

重新回顾一下公式的内涵：
$$
X = \begin{pmatrix}
  X_1^{(1)} & X_2^{(1)} & ... & X_n^{(1)} \\
  X_1^{(2)} & X_2^{(2)} & ... & X_n^{(2)} \\ 
  ... & ... & ... & ...\\ 
  X_1^{(m)} & X_2^{(m)} & ... & X_n^{(m)}
\end{pmatrix}~~;~~
W_k = \begin{pmatrix}
  W_1^{(1)} & W_2^{(1)} & ... & W_n^{(1)} \\
  W_1^{(2)} & W_2^{(2)} & ... & W_n^{(2)} \\ 
  ... & ... & ... & ...\\ 
  W_1^{(k)} & W_2^{(k)} & ... & W_n^{(k)}
\end{pmatrix}
$$
其中，$W_k$ 对应的第 $i$ 行即是通过**梯度上升法**所得到的 $i$ 阶主成分（单位向量），$X$ 的每一个样本在上面的投影即构成了降维化之后的一维；而相似的，每一阶主成分都是由原来数据集矩阵计算而得到的，可以视作是对应的 $i$ 阶特征值，也可以勾勒出原矩阵。

那么在人脸识别的技术中，如果 $X$ 可以表示一个人脸的图像，那么 $W_k$ 同样可以表示这张人脸，我们将其称作 **特征脸**。这里可以简单做一个小实验，用 KNN 算法实现人脸识别分类器。

