## 决策树

### 1. 引入

决策树 `(decision tree)` 是一类常见的非参数机器学习方法。以二分类任务为例，我们希望从给定训练数据集学习的模型用以对新示例进行分类，这个把样本分类的任务，**看做 “当前样本属于正类吗” 的决策过程** 。

决策树可以自然解决多分类问题，也可以通过对于高维平面的划分解决回归问题；同时，决策树具有一定的可解释性。



### 2. 信息熵

“信息熵” 是用以是度量样本集合纯度最常用的一种指标。信息熵越小，变量的不确定度越低。

在信息论中，信息熵一般如下定义：
$$
Ent(D) = -\sum_{i=1}^kp_i\log_2(p_i)
$$
其中，$k$ 表示样本的种类数，$p_i$ 表示第 $i$ 类样本在样本集合 $D$ 中所占的比例。此时，分类问题便可转化为：根据决策树进行划分，使得划分之后模型的信息熵最低。

我们可以将划分过程视为某种 **二叉决策树**：决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，**其他每个结点则对应于一个属性测试** ；通过信息熵降低的属性划分，每个节点所对应的样本集合将被不断细分至子节点，从而将不同类别的样本区分开来。

`playML` 中实现了一个利用信息熵、二叉树等知识实现的决策树类，利用本地库的决策树以及 `sklearn` 中决策树识别鸢尾花的代码可见 `MyDTree_test.ipynb` .



### 2*. 信息增益

`playML` 中实现的选择最优划分属性的过程，是枚举每一维特征，并计算以该特征对子样本集进行一个划分 所得到的信息熵。我们选取信息熵最小的那个作为该节点的划分属性。

令离散属性 $a:\{a^1,a^2,..,a^v\}$，$D^v$ 表示对应属性集合 $a$ 中取值为 $a^v$ 的样本集合。如果我们以属性 $a$ 对数据集 $D$ 进行一个划分，我们规定所获得的 **信息增益** 为：
$$
Gain(D,a) = Ent(D) - \sum_{v=1} \frac{|D^v|}{|D|} Ent(D^v)
$$
即 **划分前的信息熵** 减去 **划分后各分支信息熵的加权和**。一般而言，信息增益越大，则意味着使周属性来进行划分所获得的"纯度提升"越大，因此可用信息增益来进行决策树的划分属性选择。

但实际上，信息增益准则对可取值数目较多的属性（如电话号码）有所偏好，因为划分得越多，样本越 “干净”。`C4.5` 决策树采取 **增益率** 代替信息增益：
$$
Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}
\\ where~IV(a) = -\sum_{v=1}^V\frac{|D^v|}{|D|} \log_2 \frac{|D^v|}{|D|}
$$
观察形式可知，$IV(a)$ 本质上也是熵的形式，被称作属性 $a$ 的 **固有值（`(intrinsic value`）**。属性的可取值数目 $V$ 越多，$IV(a)$ 通常越大（其实也起到了一种 **规范化** 的作用）

需注意的是，增益率准则对可取值数目较少的属性有所偏好。因此 `C4.5` 算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式的算法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。



### 3. 基尼系数

基尼系数是另一个划分决策树数属性的参数：
$$
Gini(D)~ = \sum_{k=1}^m\sum_{i\neq k} p_kp_i~ = 1-\sum_{k=1}^mp_k^2
$$
直观来说， $Gini(D)$ 反映了从数据集中随机抽取两个样本，其类别标记不一致的概率。因此，$Gini(D)$ 越小，则数据集的纯度越高。

一般来说，计算基尼系数比信息熵较快（因为不用调 `log` 函数），`sklearn` 中采用的也是基尼系数作为划分的指标，但二者在大多数情况下没有效果的优劣。



### 4. 剪枝处理

我们**基于基尼系数作为划分指标**而实现的决策树其实是 `sklearn` 库中的 `CART` 决策树，同时还有很多类的决策树（如 `ID3` 决策树或 `C4` 决策树），都有着不同的划分属性准则和其他结构啥的，不太懂。

但 `CART` 决策树本身还是挺慢的：`Build_Tree` 过程的时间复杂度是 $O(nm\log m)$，而单次预测的时间复杂度也有 $O(\log m)$ . 更不用说非参数学习法也很容易产生**“过拟合”**。 因此，对决策树进行剪枝操作是很有必要的。

决策树剪枝的基本策略有 **"预剪枝"**（`pre pruning`）和 **"后剪枝"** （`post pruning`）。

预剪枝对每个结点在划分前采用 **先验估计**，若当前结点的划分不能带来树泛化性能提升，则停止划分并将当前结点标记为叶结点；也可以通过 **调整超参数** 的方式，限制决策树的内部节点过于复杂（如设置树的深度 `max_depth`，内部节点的最小样本数量 `min_samples_split`，叶子结点最大数量 `max_leaf_nodes` 等）。

后剪枝是先从训练集生成一棵完整的决策树，然后 **自底向上地对非叶结点进行考察**，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。这种操作通常比预剪枝带来较小的欠拟合风险，泛化性能也往往更优；但时间开销巨大。



### 5. 局限性

- 横平竖直的直线划分，并不能使用斜线。因此并不是最好的划分方式。
- 对个别数据敏感。这也是非参数学习的一个共性。
- 但实际上，他是后面讲的一个机器学习算法的很强大的铺垫，就是 **集成学习/随机森林**，归功于其易于产生具有差异化的模型。
