## 集成学习与随机森林

### 1. 概念

- 个体学习器：集成学习的一般结构都是先产生一组个体学习器（individual learner），在用某种策略将他们结合起来，个体学习器通常由一个现有的学习算法从训练数据中产生。

- 基学习器：如果集成中只包含同种类型的个体学习器，例如决策树集成中全都是决策树，这样的集成是 **‘同质’（homogeneous）** 的，**同质集成**中的个体学习器又称‘基学习器’，相应的学习算法又称基学习算法。

- 组件学习器：集成也可以是包含不同类型的个体学习器，例如决策树和神经网络，这样的集成是‘异质’（heterogenous）的，**异质集成**中的个体学习器称为组件学习器或者直接称为个体学习器。

集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。这对 **弱学习器（泛化性能略优于随机猜测的学习器）** 尤为明显，因此对于集成学习的研究都是针对弱学习器进行的，而基学习器有时也被直接称为弱学习器。

集成方法大体可分为两类：序列化方法（如 `AdaBoost & XGBoost`）与并行化方法（如 `Bagging & Random Forest`）。



### 2. 序列化方法（`Boosting`）

先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前学习器做的训练样本在后续受到更多的关注，然后基于调整后的样本分布来训练下一个学习器，如此重读进行，直至学习器数目达到预先指定的值。

较经典的序列化方法是 `Adaboost`，假设集成了 $T$ 个学习器 $h_t$ 及其对应的权重 $\alpha_t$，使得其加权和作为权重，能最小化损失函数：
$$
L_{exp}(H|D) = E_{x,D}[~e^{-f(x)H(x)}~]~，
\\ where~~ H(x) = \sum_T \alpha_t\cdot h_t(x)
$$
为了使得损失函数最小，我们采取 **前向分布求解算法**：第 $t$ 轮只学习一个学习器 $h_t$ 和相应的权重 $\alpha_t$，第 $t$ 轮的优化目标即为：
$$
(\alpha_t,~h_t) = \arg\min~l_{exp}(H_{t-1}+\alpha_t h_t~|~D)
$$
Boosting 算法要求基学习器对于特定的数据分布进行学习。在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重（决策树）。

而对无法接受带权样本的基学习算法（神经网络），则可通过 "重采样法"（`re-sampling`）来处理（即每一轮根据样本分布重新采样一组样本集）。

$Sklearn$ 中的 `AdaBoost` 库基于决策树桩，因此调参的时候除了集成学习的参数（如 `n_estimators` 弱学习器数量、`learning_rate` 学习率），还可以调决策树的一系列参数（如 `max_depth`、`min_samples_split`） ，代码如下：

```py
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
# 这里根据决策树的调参方式进行调参。
ada_clf = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=2), 
    n_estimators=500,)

ada_clf.fit(x_train, y_train)
ada_clf.score(x_test, y_test)
```



### 3. 并行性方法（`Bagging`）

欲得到泛化性能强的集成，集成中的个体学习器不仅应该拥有泛化能力较强于随机猜测的模型，更应该尽可能相互独立（即学习器之间需要有一定 **差异性**）。

1. 操作数据集增大差异性

最简单的想法是，给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据的子集中训练出一个基学习器。这样由于训练数据不同，我们获得的基学习器可望具有比较大的差异。

假设我们共训练了 $T$ 个二分类学习器，每个模型预测正确的可能性为 $\epsilon$，则集成学习器的预测错误的概率为：
$$
P(H(x)\neq f(x)) &= \sum_{k=0}^{T/2} \binom Tk (1-\epsilon)^k\epsilon^{T-k}
\\&\le exp(-\frac12\cdot T(1-2\epsilon)^2)
$$
可以看出两个基本结论：

- 模型的预测错误的概率会随着学习器数量增多而指数下降；
- 当 $\epsilon\ge0.5$ 后，个体集成器对于收敛是没有作用的；

给定包含 $m$ 个样本的数据集，我们先随机取出一个样本放入采样集，**再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中**。这样，经过 $m$ 次随机采样操作，我们得到若干样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。

这种采样被称为 **自主采样（`Bootstrap`）**，那些从未出现的样本占比约为：
$$
\lim_{x \to \infty} (1-\frac1n)^n = \frac1e\approx 36.6\%
$$
当然，可以直接利用这部分没有被取到的样本做测试或验证，就不需要分离测试集了。

`sklearn` 也有对应的参数 `oob` 和 `bootstrap`，详见代码：

```py
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

bagging_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500,
                               max_samples=100,
                               bootstrap=True,
                               oob_score=True)
# bootstrap=True 采用放回取样法；
# oob_score=True 使用剩下未取到的数据作为测试集。
bagging_clf.fit(x, y)
bagging_clf.oob_score_
```



2. 训练过程中添加随机变量

同时，还可以在训练学习器的过程中加入随机操作，增大个体学习器的多样性。

**随机森林（`Random Forest`，简称 RF）**  是以决策树为基学习器构建 `Bagging` 集成的基础上，进一步在决策树的训练过程中引入 **随机属性选择** 的变体。假设样本共有 $d$ 个属性，则 RF 对于决策树每一个结点的属性划分上，都随机从属性集合中选择一个 $k$ 属性子集，从中进行最优属性划分。

由于决策树极其容易受到微小扰动而产生模型的变化，因此通过这种操作得到的个体学习器间差异度明显，集成学习器具有极高的泛化性能。

还有一类 **极端随机树（`Extremely randomized trees`）**，决策树在节点划分上使用完全随机的特征和阈值。这种操作抑制了过拟合，但是也一定程度地增大了偏差，而更关注于降低方差。这种随机性也使得能够更加快速地训练。

```py
random_patches_clf = BaggingClassifier(DecisionTreeClassifier(),
                               n_estimators=500,
                               max_samples=100,
                               bootstrap=True,
                               oob_score=True,
                               n_jobs=-1,
                               max_features=4,
                               bootstrap_features=True)
# max_samples=100, bootstrap=True 表示针对样本进行随机抽样
# max_features=4, bootstrap_features=True 表示针对样本特征进行随机抽样
random_patches_clf.fit(x, y)
random_patches_clf.oob_score_
# 0.858

# 或者直接调 RF 的类
from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(n_estimators=500, 
                                random_state=666, 
                                oob_score=True, 
                                n_jobs=-1,)
rf_clf.fit(x, y)
rf_clf.oob_score_
#0.892
```



### 4. 结合策略

摸了，放个目录。

- 平均法
- 加权法
- 投票法（多数投票、加权投票）
- 学习法（Stacking）