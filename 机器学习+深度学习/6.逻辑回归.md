## 逻辑回归

### 1. 引入

对模型进行回归学习的时候可用以解决回归问题，也可用以解决分类问题。只需找一个单调可微函数将分类任务的真实标记与线性回归模型的预测值联系起来即可。譬如**二分类任务**，我们选定**单位阶跃函数**便可等价实现分类的目标。

然而单位阶跃函数不连续，因此不能直接利用。于是我们希望找到能在一定程度上近似单位阶跃函数的"替代函数" ，并希望它单调可微。**对数几率函数(logistic function)** 正是这样一个常用的替代函数：
$$
y = \frac{1}{1+e^{-x}}
$$
画图观察可知，该函数形似单位阶跃函数，且值域为 `[0,1]`，很合适作为广义线性模型的联系函数。对于训练好的一组线性回归模型所得到的 $\hat{y} = \theta^T \cdot x$，只需通过联系函数进行映射，即可求出逻辑回归的概率：
$$
\hat{p} = \sigma(\theta^T\cdot x_b)
$$
**此时对于二分类任务，当 $\hat{p}\ge0.5$ 时 $\hat{y}=1$，反之 $\hat{y} = 0$ 即可。**



### 2. 概述

- **损失函数**

尝试如下构造损失函数：当真实标签 $y=1$，此时若预测得到的 $\hat{p}$ 越小，则损失越大；当 $y=0$ 时同理。因此对于任一样本，我们可以如下定义其损失：
$$
Cost = -y\cdot log(\hat{p}) - (1-y)\cdot log(1-\hat{p})
$$
对于一个数据集 $X_b$，我们也可以表达其损失函数：
$$
J(\theta) = -\frac{1}{m}\sum(~y^{(i)}\cdot{log(\sigma(X_b^{(i)}\theta)})+(1-y^{(i)})\cdot log(1-\sigma(X_b^{(i)}\theta))~)
$$
该函数为高阶连续可导的凸函数，经典的数值优化算法（如梯度下降法）即可求出其全局最优解。



- **梯度下降法及向量化**

对于该损失函数关于任意特征的梯度，通过简单的高数推导可知：
$$
\begin{equation}
\begin{aligned}
\frac{J(\theta)}{\theta_j} 
&= \frac{1}{m}\sum(\sigma(X_b^{(i)}\theta)-y^{(i)})X_j^{(i)} \\
&= \frac{1}{m}\sum(\hat{y}^{(i)}-y^{(i)})X_j^{(i)}
\end{aligned}
\end{equation}
$$
从而可以表达出对应的梯度：
$$
\nabla J(\theta) 
= \begin{pmatrix}
\partial J/ \partial \theta_0 \\
\partial J/ \partial \theta_1  \\
...\\
\partial J/ \partial \theta_n  \\
\end{pmatrix} 
= \frac{1}{m} \begin{pmatrix}
\sum_{i=1}^m (\sigma(X_b^{(i)} \cdot \theta) - y^{(i)}) \cdot 1\\
\sum_{i=1}^m (\sigma(X_b^{(i)} \cdot \theta) - y^{(i)})\cdot X_1^{(i)} \\
...\\
\sum_{i=1}^m (\sigma(X_b^{(i)} \cdot \theta) - y^{(i)})\cdot X_n^{(i)}  \\
\end{pmatrix}
$$
类比线性回归，利用向量化对表达式化简：
$$
\nabla J(\theta) = \frac{1}{m}~X_b^T\cdot(\sigma(X_b\cdot\theta)-y)
$$

- **决策边界**

回顾逻辑回归的分类原理，当 $\hat{p}\ge0.5$ 时 $\hat{y}=1$，反之 $\hat{y} = 0$ ；观察 `Sigmoid` 函数特性，当 $\theta^T\cdot X \ge 0$ 时， $\hat{y} = 1$；反之 $\hat{y} = 0$ 。因此，可以将 $f: \theta^T\cdot X = 0$ 视作其不同决策之间的边界。

同理，我们可以通过封装一个喷涂不同区域的函数来观察 **KNN 的决策边界**。这是一个鸢尾花数据集三分类的图：

![img](https://pic3.zhimg.com/v2-23869dc69fbd157cb343cdc508c6642a_r.jpg)



### 3. 多项式特征

基于线性回归库的逻辑回归只能处理**满足线性特征的二分类问题**，具有很大的局限性。因此可以引入多项式特征，解决高阶的逻辑回归问题：`Polynomial_LogisticRegression_test.ipynb` 中给出了解决二阶分类问题的一个例子。

![img](https://pic4.zhimg.com/v2-ea949d6face9288b2654296cb905883b_r.jpg)

同理，如果我们设置的阶数（degree）过大，也会发生过拟合的问题。因此正则化必不可少。在逻辑回归中，我们如下表示模型泛化的时候提到的 L1 正则、L2 正则化：
$$
C\cdot J(\theta)+L_2,~~C\cdot J(\theta)+L_1
$$
调小超参数 $C$ 的值，表示越对参数的取值进行限制。可以引入 `sklearn` 中的逻辑回归库，通过管道封装为一个函数：

```py
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression

def PolynomiaLogisticRegression(degree, C, penalty='l2'):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scale', StandardScaler()),
        ('log_reg', LogisticRegression(C=C, penalty=penalty))
    ])
```



### 4. 多分类回归问题

在开始之初，说逻辑回归只可以解决二分类问题， 其实可以稍加改造使其能够解决多分类问题。当然这个改造方式并不是只针对逻辑回归这一种算法，这是一种通用的近乎于可以改造所有的二分类。

- `One vs. Rest` ：对于 n 分类的问题，我们每次将一种类别单拿出来，将其他所有类别视为一体，便转化为了二分类问题；对于每一个二分类任务，我们可以得到共一个分类的概率，我们取 n 个得分（概率）中最高的那个作为预测结果；

- `One vs One`：在 n 个类别中，每次都挑出其中 2 个来解决二分类任务，判断在这两个种类中样本更偏向于那一种类别；对于所有的类别选取情况，也就是 $C(n，2)$ 个情况里依次操作，最后选择获胜数最高的那个分类作为最终的预测结果；

`OvO` 相较于 `OvR`，**其准确率会更高一些，同时耗时也会提高**。利用`OvR or OvO`进行多分类回归问题，可参考代码 `OVR & OVO_test.ipynb`。`sklearn` 库中的逻辑回归已有了支持多分类任务的函数。只需要在参数中加入 `multi-class="ovr"` 即可。

同时，`sklearn` 中也封装了这两个多分类器：

```py
from sklearn.multiclass import OneVsRestClassifier
ovr = OneVsRestClassifier(log_reg)

from sklearn.multiclass import OneVsOneClassifier
ovo = OneVsOneClassifier(log_reg)
# 将二分类模型转化为多分类模型（通过OvO/OvR）

ovr.fit(X_train, y_train)
ovr.score(X_test, y_test)
```

